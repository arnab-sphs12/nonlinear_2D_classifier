{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-linear 2D classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sklearn.datasets as datasets\n",
    "from sklearn.datasets import make_moons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate the dataset\n",
    "X, y = make_moons(n_samples=2000, noise=0.2, random_state=42)\n",
    "\n",
    "# # Visualize the dataset\n",
    "# plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], label=\"Class A\", s=10)\n",
    "# plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], label=\"Class B\", s=10)\n",
    "# plt.xlabel(\"Feature 1\")\n",
    "# plt.ylabel(\"Feature 2\")\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "X = X.astype(np.float32)\n",
    "y = y.astype(np.float32)\n",
    "\n",
    "print(X.dtype)\n",
    "print(y.dtype)\n",
    "\n",
    "y = y.reshape((len(y),1))\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "X, y = shuffle(X, y, random_state=42)  # You can set a random seed for reproducibility\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1], c=y, s=10)\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "input_dim =2 \n",
    "output_dim = 1\n",
    "\n",
    "W1 = tf.Variable(initial_value=tf.random.uniform(shape=(input_dim,32), minval=-0.01, maxval=0.01))\n",
    "# display(W1)\n",
    "b1 = tf.Variable(initial_value=tf.zeros(32))\n",
    "# display(b1)\n",
    "\n",
    "W2 = tf.Variable(initial_value=tf.random.uniform(shape=(32,16), minval=-0.01, maxval=0.01))\n",
    "# display(W2)\n",
    "b2 = tf.Variable(initial_value=tf.zeros(16))\n",
    "# display(b2)\n",
    "\n",
    "W3 = tf.Variable(initial_value=tf.random.uniform(shape=(16,output_dim), minval=-0.01, maxval=0.01))\n",
    "# display(W3)\n",
    "b3 = tf.Variable(initial_value=tf.zeros(output_dim))\n",
    "# display(b3)\n",
    "\n",
    "def model(inputs):\n",
    "    # return tf.matmul(inputs,W)+b\n",
    "    y1 = tf.nn.relu(tf.matmul(inputs,W1)+b1)\n",
    "    y2 = tf.nn.relu(tf.matmul(y1,W2)+b2)\n",
    "    # return tf.nn.relu(tf.matmul(inputs,W)+b)\n",
    "    return tf.nn.sigmoid(tf.matmul(y2,W3)+b3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def sq_loss(targets, predictions):\n",
    "#     per_sample_loss = tf.square(targets-predictions)\n",
    "#     return tf.reduce_mean(per_sample_loss)\n",
    "# learning_rate = 0.001\n",
    "\n",
    "# def training_step(inputs, targets):\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         predictions = model(inputs)\n",
    "#         loss = sq_loss(targets, predictions)\n",
    "#     grad_loss_W1, grad_loss_b1, grad_loss_W2, grad_loss_b2, grad_loss_W3, grad_loss_b3  = tape.gradient(loss, [W1,b1,W2,b2,W3,b3])\n",
    "#     W1.assign_sub(grad_loss_W1*learning_rate)\n",
    "#     b1.assign_sub(grad_loss_b1*learning_rate)\n",
    "#     W2.assign_sub(grad_loss_W2*learning_rate)\n",
    "#     b2.assign_sub(grad_loss_b2*learning_rate)\n",
    "#     W3.assign_sub(grad_loss_W3*learning_rate)\n",
    "#     b3.assign_sub(grad_loss_b3*learning_rate)\n",
    "#     return loss\n",
    "\n",
    "\n",
    "# Training loop\n",
    "learning_rate = 0.001\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "def training_step(inputs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs)\n",
    "\n",
    "        #loss calculation\n",
    "        per_sample_loss = tf.square(targets-predictions)\n",
    "        loss = tf.reduce_mean(per_sample_loss)\n",
    "        # loss = sq_loss(targets, predictions)\n",
    "\n",
    "    # obtain the gradients \n",
    "    gradients = tape.gradient(loss, [W1, b1, W2, b2, W3, b3])\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, [W1, b1, W2, b2, W3, b3]))\n",
    "\n",
    "    return loss\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "loss_t = []\n",
    "tsteps = 4000\n",
    "pbar = tqdm.tqdm(total=tsteps, desc='Training Progress')\n",
    "for step in tqdm.trange(tsteps):\n",
    "    loss = training_step(X, y)\n",
    "    loss_t.append(loss)\n",
    "    pbar.update(1)\n",
    "    pbar.set_postfix({'Loss': f'{loss:.4f}', 'Accuracy': f'{1.0 - loss:.4f}'})\n",
    "    # print(f'loss at step {step}: {loss:.4f}')\n",
    "\n",
    "plt.plot(loss_t)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# from IPython.display import display, clear_output\n",
    "\n",
    "# loss_t = []\n",
    "# tsteps = 400\n",
    "\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(1, 1, 1) \n",
    "# ax.tick_params(axis='both', labelsize=10, color='white', labelcolor='white')\n",
    "\n",
    "# pbar = tqdm.tqdm(total=tsteps, desc='Training Progress')\n",
    "# for step in range(tsteps):\n",
    "#     loss = training_step(X, y)\n",
    "#     loss_t.append(loss)\n",
    "\n",
    "#     ax.set_xlim(0, step)    \n",
    "#     ax.cla()\n",
    "#     ax.plot(loss_t)\n",
    "#     # ax.xaxis.label.set_color('white')\n",
    "#     display(fig)    \n",
    "#     clear_output(wait = True)\n",
    "#     plt.pause(1.0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "predictions = model(X)\n",
    "\n",
    "print(predictions[0:10])\n",
    "\n",
    "# input = np.linspace(-1, 2, 100)\n",
    "# out = np.linspace(0, 0, 100)\n",
    "# print(input)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=predictions[:, 0], s=10, vmin=0, vmax=1)\n",
    "# plt.plot(input, out, color='red')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflowV2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
